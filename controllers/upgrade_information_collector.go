package controllers

import (
	"context"
	"fmt"
	"strconv"
	"time"

	configv1 "github.com/openshift/api/config/v1"
	machineconfigurationv1 "github.com/openshift/machine-config-operator/pkg/apis/machineconfiguration.openshift.io/v1"
	"github.com/prometheus/client_golang/prometheus"
	apimeta "k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/util/sets"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	managedupgradev1beta1 "github.com/appuio/openshift-upgrade-controller/api/v1beta1"
	"github.com/appuio/openshift-upgrade-controller/pkg/clusterversion"
	"github.com/appuio/openshift-upgrade-controller/pkg/healthcheck"
)

//+kubebuilder:rbac:groups=config.openshift.io,resources=clusterversions,verbs=get;list;watch;update;patch
//+kubebuilder:rbac:groups=machineconfiguration.openshift.io,resources=machineconfigpools,verbs=get;list;watch;update;patch

var clusterUpgradingDesc = prometheus.NewDesc(
	MetricsNamespace+"_cluster_upgrading",
	"Set to 1 if the cluster is currently upgrading, 0 otherwise.",
	[]string{},
	nil,
)

var poolsUpgradingDesc = prometheus.NewDesc(
	MetricsNamespace+"_machine_config_pools_upgrading",
	"Set to 1 if a machine config pool in the cluster is currently upgrading, 0 otherwise. Paused pools are not considered upgrading.",
	[]string{"pool"},
	nil,
)

var poolsPausedDesc = prometheus.NewDesc(
	MetricsNamespace+"_machine_config_pools_paused",
	"Set to 1 if a machine config pool in the cluster is currently paused, 0 otherwise.",
	[]string{"pool"},
	nil,
)

var jobStateDesc = prometheus.NewDesc(
	MetricsNamespace+"_upgradejob_state",
	"Returns the state of jobs in the cluster. 'pending', 'active', 'succeeded', or 'failed' are possible states. Final states may have a reason.",
	[]string{
		"upgradejob",
		"start_after",
		"start_before",
		"desired_version_force",
		"desired_version_image",
		"desired_version_version",
		"state",
		"reason",
		"matches_disruptive_hooks",
	},
	nil,
)

var jobStartAfterDesc = prometheus.NewDesc(
	MetricsNamespace+"_upgradejob_start_after_timestamp_seconds",
	"The value of the startAfter field of the job.",
	[]string{
		"upgradejob",
	},
	nil,
)

var jobStartBeforeDesc = prometheus.NewDesc(
	MetricsNamespace+"_upgradejob_start_before_timestamp_seconds",
	"The value of the startBefore field of the job.",
	[]string{
		"upgradejob",
	},
	nil,
)

var upgradeConfigInfoDesc = prometheus.NewDesc(
	MetricsNamespace+"_upgradeconfig_info",
	"Information about the upgradeconfig object",
	[]string{
		"upgradeconfig",
		"cron",
		"location",
		"suspended",
	},
	nil,
)

var upgradeConfigNextPossibleScheduleDesc = prometheus.NewDesc(
	MetricsNamespace+"_upgradeconfig_next_possible_schedule_timestamp_seconds",
	"The value of the time field of the next possible schedule for an upgrade.",
	[]string{
		"upgradeconfig",
		"n",
		"timestamp",
	},
	nil,
)

// UpgradeInformationCollector is a Prometheus collector that exposes various metrics about the upgrade process.
type UpgradeInformationCollector struct {
	client.Client

	ManagedUpstreamClusterVersionName string
}

var _ prometheus.Collector = &UpgradeInformationCollector{}

// Describe implements prometheus.Collector.
// Sends the static description of the metrics to the provided channel.
func (*UpgradeInformationCollector) Describe(ch chan<- *prometheus.Desc) {
	ch <- clusterUpgradingDesc
	ch <- poolsUpgradingDesc
	ch <- poolsPausedDesc
	ch <- jobStateDesc
	ch <- jobStartAfterDesc
	ch <- jobStartBeforeDesc
	ch <- upgradeConfigInfoDesc
	ch <- upgradeConfigNextPossibleScheduleDesc
}

// Collect implements prometheus.Collector.
// Sends a metric if the cluster is currently upgrading and an upgrading metric for each machine config pool.
// It also collects job states and whether they have matching disruptive hooks.
func (m *UpgradeInformationCollector) Collect(ch chan<- prometheus.Metric) {
	ctx := context.Background()

	mcpl := machineconfigurationv1.MachineConfigPoolList{}
	if err := m.Client.List(ctx, &mcpl); err != nil {
		err := fmt.Errorf("failed to list machine config pools: %w", err)
		ch <- prometheus.NewInvalidMetric(clusterUpgradingDesc, err)
		ch <- prometheus.NewInvalidMetric(poolsUpgradingDesc, err)
		ch <- prometheus.NewInvalidMetric(poolsPausedDesc, err)
	}
	poolsUpdating := healthcheck.MachineConfigPoolsUpdating(mcpl)
	pus := sets.NewString()
	for _, p := range poolsUpdating {
		if p.Paused {
			continue
		}
		pus.Insert(p.Name)
	}
	for _, mcp := range mcpl.Items {
		ch <- prometheus.MustNewConstMetric(
			poolsUpgradingDesc,
			prometheus.GaugeValue,
			boolToFloat64(pus.Has(mcp.Name)),
			mcp.Name,
		)
		ch <- prometheus.MustNewConstMetric(
			poolsPausedDesc,
			prometheus.GaugeValue,
			boolToFloat64(mcp.Spec.Paused),
			mcp.Name,
		)
	}

	var cv configv1.ClusterVersion
	if err := m.Get(ctx, client.ObjectKey{Name: m.ManagedUpstreamClusterVersionName}, &cv); err != nil {
		ch <- prometheus.NewInvalidMetric(clusterUpgradingDesc, err)
	} else {
		ch <- prometheus.MustNewConstMetric(
			clusterUpgradingDesc,
			prometheus.GaugeValue,
			boolToFloat64(!clusterversion.IsVersionUpgradeCompleted(cv) || pus.Len() > 0),
		)
	}

	var configs managedupgradev1beta1.UpgradeConfigList
	if err := m.Client.List(ctx, &configs); err != nil {
		ch <- prometheus.NewInvalidMetric(upgradeConfigNextPossibleScheduleDesc, fmt.Errorf("failed to list upgrade jobs: %w", err))
	} else {
		for _, config := range configs.Items {
			ch <- prometheus.MustNewConstMetric(
				upgradeConfigInfoDesc,
				prometheus.GaugeValue,
				1,
				config.Name,
				config.Spec.Schedule.Cron,
				config.Spec.Schedule.Location,
				strconv.FormatBool(config.Spec.Schedule.Suspend),
			)
			for i, nps := range config.Status.NextPossibleSchedules {
				ch <- prometheus.MustNewConstMetric(
					upgradeConfigNextPossibleScheduleDesc,
					prometheus.GaugeValue,
					float64(nps.Time.Unix()),
					config.Name,
					strconv.Itoa(i),
					nps.Time.UTC().Format(time.RFC3339),
				)
			}
		}
	}

	var jobs managedupgradev1beta1.UpgradeJobList
	if err := m.Client.List(ctx, &jobs); err != nil {
		ch <- prometheus.NewInvalidMetric(jobStateDesc, fmt.Errorf("failed to list upgrade jobs: %w", err))
		ch <- prometheus.NewInvalidMetric(jobStartAfterDesc, fmt.Errorf("failed to list upgrade jobs: %w", err))
		ch <- prometheus.NewInvalidMetric(jobStartBeforeDesc, fmt.Errorf("failed to list upgrade jobs: %w", err))
		return
	}

	var jobsHooks managedupgradev1beta1.UpgradeJobHookList
	if err := m.Client.List(ctx, &jobsHooks); err != nil {
		ch <- prometheus.NewInvalidMetric(jobStateDesc, fmt.Errorf("failed to list upgrade job hooks: %w", err))
		ch <- prometheus.NewInvalidMetric(jobStartAfterDesc, fmt.Errorf("failed to list upgrade jobs: %w", err))
		ch <- prometheus.NewInvalidMetric(jobStartBeforeDesc, fmt.Errorf("failed to list upgrade jobs: %w", err))
		return
	}

	for _, job := range jobs.Items {
		v := job.Spec.DesiredVersion
		if v == nil {
			v = &configv1.Update{}
		}
		ch <- prometheus.MustNewConstMetric(
			jobStateDesc,
			prometheus.GaugeValue,
			1,
			job.Name,
			job.Spec.StartAfter.UTC().Format(time.RFC3339),
			job.Spec.StartBefore.UTC().Format(time.RFC3339),
			strconv.FormatBool(v.Force),
			v.Image,
			v.Version,
			jobState(job),
			jobStateReason(job),
			strconv.FormatBool(jobHasMatchingDisruptiveHook(job, jobsHooks)),
		)
		ch <- prometheus.MustNewConstMetric(
			jobStartAfterDesc,
			prometheus.GaugeValue,
			float64(job.Spec.StartAfter.Unix()),
			job.Name,
		)
		ch <- prometheus.MustNewConstMetric(
			jobStartBeforeDesc,
			prometheus.GaugeValue,
			float64(job.Spec.StartBefore.Unix()),
			job.Name,
		)
	}
}

func boolToFloat64(b bool) float64 {
	if b {
		return 1
	}
	return 0
}

// jobStateReason returns the reason for the current state of the job.
// All final states should have a reason.
func jobStateReason(job managedupgradev1beta1.UpgradeJob) string {
	sc := apimeta.FindStatusCondition(job.Status.Conditions, managedupgradev1beta1.UpgradeJobConditionSucceeded)
	if sc != nil && sc.Status == metav1.ConditionTrue {
		return sc.Reason
	}
	sf := apimeta.FindStatusCondition(job.Status.Conditions, managedupgradev1beta1.UpgradeJobConditionFailed)
	if sf != nil && sf.Status == metav1.ConditionTrue {
		return sf.Reason
	}
	return ""
}

func jobState(job managedupgradev1beta1.UpgradeJob) string {
	if apimeta.IsStatusConditionTrue(job.Status.Conditions, managedupgradev1beta1.UpgradeJobConditionSucceeded) {
		return "succeeded"
	} else if apimeta.IsStatusConditionTrue(job.Status.Conditions, managedupgradev1beta1.UpgradeJobConditionFailed) {
		return "failed"
	} else if apimeta.IsStatusConditionTrue(job.Status.Conditions, managedupgradev1beta1.UpgradeJobConditionPaused) {
		return "paused"
	} else if apimeta.IsStatusConditionTrue(job.Status.Conditions, managedupgradev1beta1.UpgradeJobConditionStarted) {
		return "active"
	}
	return "pending"
}

func jobHasMatchingDisruptiveHook(job managedupgradev1beta1.UpgradeJob, hooks managedupgradev1beta1.UpgradeJobHookList) bool {
	for _, hook := range hooks.Items {
		sel, err := metav1.LabelSelectorAsSelector(&hook.Spec.Selector)
		if err != nil {
			log.Log.Error(err, "failed to parse hook selector")
			continue
		}
		if !sel.Matches(labels.Set(job.Labels)) {
			continue
		}
		if hook.WouldExecute(&job) && hook.Spec.Disruptive {
			return true
		}
	}

	return false
}
